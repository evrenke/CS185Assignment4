WARNING: the parallel command execution for Question 5 did not work, and caused the IBM server connection from PuTTY to crash.
Despite severely limiting the scpoe of the bash script for question 4, I was unable to figure out the cause of the problem.

QUESTION 1) Get top 100 most helpful voted reviews, and put them in REVIEWS directory as a .txt file

The following commands take about 12 min to execute

 1993  cut -f 3,9 amazon_reviews_us_Books_v1_02.tsv | sort -nk2 | head -n 100 | awk '{print $1}' > reviewIDs.txt
 1994  input="reviewIDs.txt"; while read -r line; do awk -F"\t" -v "ID=$line" '$3 == ID' amazon_reviews_us_Books_v1_02.tsv > "REVIEWS/$line.txt"; done < "$input"
 
QUESTION 2) Remove "ing", "ed", and "s" endings from all words

This is a single piped command, and is very fast to execute
 
 1995  for FILE in REVIEWS/*.txt; do cut -f 14 "$FILE" | sed -e 's/ing[[:blank:]]/ /g;s/ed[[:blank:]]/ /g;s/[Ss][[:blank:]]/ /g' > "$FILE.temp" ; mv "$FILE.temp" "$FILE" ;  done

QUESTION 3) Remove commas,dots, and all other special symbols from the review, and then remove 1 and 2 letter words, and a list of common words from the review

I used two commands, one for symbol removal, and one for word removal.
The symbol removal cleans all special symbols from the reviews
The word removal gets rid of all 1 and 2 letter words, and then a list of "stop words" from this list:
https://blog.hubspot.com/marketing/stop-words-seo

 
 1996  for FILE in REVIEWS/*.txt; do sed -e 's/,//g;s/\.//g;s/\;//g;s/\x27//g;s/\!//g;s/\://g;s/\"//g;s/\?//g;s/\///g;s/\\//g;s/'\{'//g;s/'\}'//g;s/\[//g;s/\]//g;s/\|//g;s/\<//g;s/\>//g;s/\~//g;s/\`//g;s/@//g;s/\#//g;s/\$//g;s/%//g;s/\^//g;s/\&//g;s/\*//g;s/(//g;s/)//g;s/-//g;s/_//g;s/=//g;s/+//g' "$FILE" > "$FILE.temp" ; mv "$FILE.temp" "$FILE" ;  done
 1997  for FILE in REVIEWS/*.txt; do sed -e 's/\<.\>//g;s/\<..\>//g' "$FILE" | sed 's/\<about\>//g;s/\<actually\>//g;s/\<almost\>//g;s/\<also\>//g;s/\<although\>//g;s/\<always\>//g;s/\<and\>//g;s/\<any\>//g;s/\<are\>//g;s/\<became\>//g;s/\<become\>//g;s/\<but\>//g;s/\<can\>//g;s/\<could\>//g;s/\<did\>//g;s/\<does\>//g;s/\<each\>//g;s/\<either\>//g;s/\<else\>//g;s/\<for\>//g;s/\<from\>//g;s/\<had\>//g;s/\<has\>//g;s/\<have\>//g;s/\<hence\>//g;s/\<how\>//g;s/\<its\>//g;s/\<may\>//g;s/\<maybe\>//g;s/\<might\>//g;s/\<mine\>//g;s/\<must\>//g;s/\<neither\>//g;s/\<nor\>//g;s/\<not\>//g;s/\<when\>//g;s/\<where\>//g;s/\<wherea\>//g;s/\<wherever\>//g;s/\<whenever\>//g;s/\<whether\>//g;s/\<which\>//g;s/\<while\>//g;s/\<who\>//g;s/\<whom\>//g;s/\<whoever\>//g;s/\<whose\>//g;s/\<why\>//g;s/\<will\>//g;s/\<with\>//g;s/\<within\>//g;s/\<without\>//g;s/\<would\>//g;s/\<yes\>//g;s/\<yet\>//g;s/\<you\>//g;s/\<your\>//g;s/\<the\>//g;s/\<thi\>//g' > "$FILE.temp" ; mv "$FILE.temp" "$FILE" ; done
 

QUESTION 4) Write a shell script that takes 2 filenames as parameters: REVIEWS/reviewID.txt and the tweets file you downloaded above (training*.csv), 
compares each tweet text (6th column) against the review_body, 
and appends to the end of the REVIEWS/reviewID.txt file all tweets that have at least 2 words in common with the review_body.

This script uses both an Amazon book review AND a tweet from a given database.
The tweets were obtained using the database command below:
wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip
The script itself does the same special symbol cleanup as above in QUESTION 2.



 1998  rm commonTweets.sh
 1999  nano commonTweets.sh
 2000  chmod +x commonTweets.sh
 2001  cp REVIEWS/R100HYLFOO4XDG.txt testreview.txt
 2002  time ./commonTweets.sh testreview.txt training.1600000.processed.noemoticon.csv

--- skipping command numbers here due to repetition

QUESTION 5) Use the parallel command we have seen to perform this operation against all 100 REVIEWS/reviewID.txt files in parallel. 
Time your parallel execution with the time command. How long did it take?
This is the failed answer to this assignment. 
For some reason, despite limitations on both the scope of the bash script, 
and the number of processes parallel is allowed to use,
it causes memory usage problems for the IBM server.

 2008  time find ./REVIEWS -type f -name "*.txt" | xargs -P 5 -I % ./commonTweets.sh % training.1600000.processed.noemoticon.csv
 
QUESTION 6) Amongst all the tweets you filtered out as similar to these 100 REVIEWS/reviewID.txt files, count the word occurrences. 
Which 10 tweet words appear most frequently overall? 
Note: if your most frequent words are insignificant stop words (like "if" "or" "and", try removing those first and repeating).

For this answer, given that QUESTION 5 had worked properly, this piped command would loop through all reviews and their relavant tweets, 
and remove common words and give the top 10 twwet words by uniq frequency
 
 2009  for FILE in REVIEWS/*.txt; do sed "1 d" "$FILE" | sed 's/\<.\>//g;s/\<..\>//g' | sed 's/\<about\>//g;s/\<actually\>//g;s/\<almost\>//g;s/\<also\>//g;s/\<although\>//g;s/\<always\>//g;s/\<and\>//g;s/\<any\>//g;s/\<are\>//g;s/\<became\>//g;s/\<become\>//g;s/\<but\>//g;s/\<can\>//g;s/\<could\>//g;s/\<did\>//g;s/\<does\>//g;s/\<each\>//g;s/\<either\>//g;s/\<else\>//g;s/\<for\>//g;s/\<from\>//g;s/\<had\>//g;s/\<has\>//g;s/\<have\>//g;s/\<hence\>//g;s/\<how\>//g;s/\<its\>//g;s/\<may\>//g;s/\<maybe\>//g;s/\<might\>//g;s/\<mine\>//g;s/\<must\>//g;s/\<neither\>//g;s/\<nor\>//g;s/\<not\>//g;s/\<when\>//g;s/\<where\>//g;s/\<wherea\>//g;s/\<wherever\>//g;s/\<whenever\>//g;s/\<whether\>//g;s/\<which\>//g;s/\<while\>//g;s/\<who\>//g;s/\<whom\>//g;s/\<whoever\>//g;s/\<whose\>//g;s/\<why\>//g;s/\<will\>//g;s/\<with\>//g;s/\<within\>//g;s/\<without\>//g;s/\<would\>//g;s/\<yes\>//g;s/\<yet\>//g;s/\<you\>//g;s/\<your\>//g;s/\<the\>//g;s/\<thi\>//g'| sed 's/\s\+/\n/g' | sort -r | uniq -c | sort -nrk 1 | head -n 11 | tail -n 10 > "$FILE.top10" ;  done

QUESTION 7) Repeat the same analysis for any 100 customer reviews with low helpfulness scores of 0. 
Save them under directory REVIEWS_UNHELPFUL in files named as REVIEWS_UNHELPFUL/reviewID.txt .

For this question, all commands are the same as the first 6 questions, except 2 differences.
Only key difference is the beginning list of IDs is obtained by tail instead of head, shown below
The second difference is all references to "REVIEWS" is replaced with "REVIEWS_UNHELPFUL".

2010  cut -f 3,9 amazon_reviews_us_Books_v1_02.tsv | sort -nk2 | tail -n 100 | awk '{print $1}' > reviewIDs.txt
